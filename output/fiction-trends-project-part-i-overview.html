<!DOCTYPE html>
<html lang="en">

<head>
  <!-- Required meta tags always come first -->
  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>Fiction trends project. Part I: Overview | Uncontrolled Experiment
</title>
  <link rel="canonical" href="sallymit.com/fiction-trends-project-part-i-overview.html">

  <link rel="alternate" type="application/atom+xml" href="sallymit.com/feeds/all.atom.xml" title="Full Atom Feed">
  <link rel="alternate" type="application/atom+xml" href="sallymit.com/feeds/projects.atom.xml" title="Categories Atom Feed">

  <link rel="apple-touch-icon" href="sallymit.com/apple-touch-icon.png" sizes="180x180">
  <link rel="icon" type="image/png" href="sallymit.com/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="sallymit.com/favicon-16x16.png" sizes="16x16">
  <link rel="manifest" href="sallymit.com/manifest.json">
  <meta name="theme-color" content="#333333">

  <link rel="stylesheet" href="sallymit.com/theme/css/bootstrap.min.css">
  <link rel="stylesheet" href="sallymit.com/theme/css/font-awesome.min.css">
  <link rel="stylesheet" href="sallymit.com/theme/css/pygments/monokai.min.css">
  <link rel="stylesheet" href="sallymit.com/theme/css/style.css">


<meta name="description" content="This post is about the capstone project I developed during Data Science Immersive program at Galvanize">
</head>

<body>
  <header class="header">
    <div class="container">
      <div class="row">
        <div class="col-sm-4">
          <a href="sallymit.com/">
            <img class="img-fluid" src=sallymit.com/images/profile.jpg width=200 height=200 alt="Uncontrolled Experiment">
          </a>
        </div>
        <div class="col-sm-8">
          <h1 class="title"><a href="sallymit.com/">Uncontrolled Experiment</a></h1>
          <p class="text-muted">Non-random and Totally Biased Blog</p>
          <ul class="list-inline">
            <li class="list-inline-item"><a class="fa fa-github" href="https://github.com/SallyM" target="_blank"></a></li>
            <li class="list-inline-item"><a class="fa fa-linkedin" href="http://linkedin.com/in/sallymit" target="_blank"></a></li>
          </ul>
        </div>
      </div>
    </div>
  </header>

  <div class="main">
    <div class="container">
      <h1>Fiction trends project. Part I: Overview
</h1>
      <hr>
<article class="article">
  <header>
    <ul class="list-inline">
      <li class="list-inline-item text-muted" title="2017-01-22T20:00:00-08:00">
        <i class="fa fa-clock-o"></i>
        Sun 22 January 2017
      </li>
      <li class="list-inline-item">
        <i class="fa fa-folder-open-o"></i>
        <a href="sallymit.com/category/projects.html">Projects</a>
      </li>
      <li class="list-inline-item">
        <i class="fa fa-files-o"></i>
        <a href="sallymit.com/tag/natural-language-processing.html">#Natural Language Processing</a>      </li>
    </ul>
  </header>
  <div class="content">
    <h1>Overview</h1>
<p>Every month Amazon Kindle Direct Publishing sends a newsletter to its indie authors with news, updates, interesting articles, and success stories to encourage authors to write and publish more. The success stories are fascinating and inspiring, but authors don’t always know which types of books sell well and which don’t, so every success story just seems to be pure luck. In this project I applied data science to report on the trends of the publishing world over the years. The end goal is to help authors who want to make a living with their writing make informed writing decisions.</p>
<h1>Data</h1>
<p>Data collection and cleaning (in a never ending loop) took most of the two weeks alloted at the end of the course for the project. It involved reaching out to several people in hopes of easily obtaining the data, and when that failed -- scraping New York Times for historic bestseller lists, scraping Goodreads for descriptions and reviews of books on the lists and extracting several thousand reviews out of millions in the Amazon data dump from a few years ago for the Stanford study. I learned about parsing pdfs (hint: it’s a pain in the neck!), pros and cons of APIs, web-crawling, and merging of all the data.</p>
<p>I was hoping to go way back in time with the books, but after a lot of trial and error was forced to stick to the recent lists history, from 2008 to present, in order to meet project requirements and have time to make an MVP. My biggest lessons at this point were about trying many things in quick succession on a small scale before unleashing them on the whole huge dataset, not forgetting to check the results to avoid nasty surprises (got many of those <em>sigh</em>), and not getting hung up on trying to fix things that. Just. Don’t. Work.</p>
<h1>Analysis</h1>
<p>After settling on the data I was going to use, I moved on to my analysis strategy. This was a LOT easier than collecting the data had been. Using python’s nltk package I tokenized my descriptions and reviews, removed stop words, and vectorized what was left based on term and document frequencies, using tf-idf. I then fed the output to scikit-learn’s non-negative matrix factorization (NMF) module to extract topics out of the available text. After that I used the scikit-learn k-means clustering algorithm to cluster books based on the topics obtained in the previous step.</p>
<p>This took a little bit of trial and error. While clustering, I discovered that my vectorizer considered words such as ‘book’, ‘author’, ‘novel’, ‘series’, ‘read’ (and several others that included URLs) important, while they were not actually very useful when eye-balling the topics. I ended up tinkering with the models by adding some words to the stop-words list, tweaking the number of topics to be extracted, and slightly changing the way clusters were calculated.</p>
<h1>Results</h1>
<p>The results were pretty fascinating! My model came up with 6 clusters which included:</p>
<ul>
<li>family, life, mother, year, home, mallery,  town, child, sister, carr<ul>
<li>e.g., THREE SISTERS by Susan Mallery</li>
</ul>
</li>
<li>want, see, say, doe, world, feel, vampire, come, people, hero<ul>
<li>e. g., ALL I WANT FOR CHRISTMAS IS A VAMPIRE by Kerrelyn Sparks</li>
</ul>
</li>
<li>higgins, barrington, john, reacher, logan, president, case, cia, johansen, clark<ul>
<li>e. g., LEAN MEAN THIRTEEN by Janet Evanovich</li>
</ul>
</li>
<li>line, keep, hard, however, every, main, action, lot, wait, start<ul>
<li>e.g., YOUR HEART BELONGS TO ME by Dean Koontz</li>
</ul>
</li>
<li>nora, robert, romance, suspense, witch, brother, ireland, evil, parker, six     <ul>
<li>e.g., CHARMED AND ENCHANTED by Nora Roberts</li>
</ul>
</li>
<li>patterson, james, cross, alex, murder, private, club, killer, case, michael    <ul>
<li>e.g., CROSS FIRE by James Patterson</li>
</ul>
</li>
</ul>
<p>Hmm.</p>
<p>Turns out the ‘action’ cluster peaked in 2010, and then experienced a very sharp decline. This cluster included books about zombies, virus outbreaks and general world calamities. </p>
<p>Vampire cluster included all kinds of vampire books as well as others in the paranormal (mostly romance) genres. Ugh. All these undead creatures are <em>finally</em> dying. </p>
<p>What looks to be really on the rise are detective stories, spy novels and psychological thrillers (Girl on the Train, anyone?). Friend/family genre seems to be doing quite well, too.</p>
<p>Here is a plot of these clusters to help you see:</p>
<p><img alt="Clusters" class="img-fluid" src="images/clusters.png" title="Clusters"/></p>
<p>But look at the two boring ones at the bottom! These are the ‘Nora Roberts’ and ‘James Patterson’ clusters. Yes. These two authors each got their own cluster. While they don’t have any crazy ups and downs between 2008 and 2016, these two have steadily been on the bestseller lists for the eight years of data that I was able to gather. My clusters are far from perfect as you can probably tell from the words which were considered descriptive of some of them, but my model INSISTED (no matter how many different ways I tried to run it) that Nora and James be their own clusters. Nora’s cluster had 50 books, 46 of which were by her. 1 was by J.D. Robb, who, in case you didn’t know, is Nora in disguise :)</p>
<h1>Future work</h1>
<p>As I only had two weeks to complete this project, there were lots of things I had to cut in order to fit the timeline. Here are some plans I have for the future:
1. Obtain more bestseller lists from New York Times (pdf parser, I’ll be back!) and other organizations such as Publishers Weekly, USA Today, etc. This can hopefully include a larger variety of books and be more interesting overall because of more historic data.
2. Use numerical data, such as actual ranking of books on the lists, in analysis. So far I only analyzed the textual part of my data, but I suspect that including current and past numeric ranking of books and number of weeks on each book was present on the list in my analysis would lead to more accurate trends and better clustering.
3. Include other textual data, such as character descriptions of books, reviews from blogs and newspapers, tweets and other social media. This would create a more complete picture of every genre and would reveal what people like/hate about certain characters, setting, plots, themes, etc.
4. Make a web-app to process new data in real, or close to real, time. I’m hoping to create something that can be accessed any time with most recent information and trends available at a few clicks. Also, an app can make it easier to interpret all the interesting findings of my models without having to rely on somewhat confusing plots. </p>
<h1>Conclusion</h1>
<p>All in all, this was a fun and highly educational experience for me. Things I’ve learned are:
1. Planning matters. It is easier to complete a project when you have a plan, even a bad one.
2. Trial and error on small parts first will prevent you from having to clean up a big mess later.
3. Tmux is your friend! I had to get reviews for about 3000 books from Goodreads. Trust me. Tmux is the best thing in the world.
4. Always be commiting!!! Learned this the hard way. Nearly lost a huge chunk of my data. The only thing that saved me was printing that data to the console two minutes before accidentally deleting the file.
5. Write more and be your own cluster! This is not a data science lesson, but still an important one for those of us who like to write :)
In the next post I will describe my battles with data collection and cleaning for this project.</p>
<p>Until then!</p>
  </div>
</article>
    </div>
  </div>

  <footer class="footer">
    <div class="container">
      <div class="row">
       <ul class="col-sm-6 list-inline">
          <li class="list-inline-item"><a href="sallymit.com/archives.html">Archives</a></li>
          <li class="list-inline-item"><a href="sallymit.com/categories.html">Categories</a></li>
          <li class="list-inline-item"><a href="sallymit.com/tags.html">Tags</a></li>
        </ul>
        <p class="col-sm-6 text-sm-right text-muted">
          Generated by <a href="https://github.com/getpelican/pelican" target="_blank">Pelican</a> / <a href="https://github.com/nairobilug/pelican-alchemy" target="_blank">&#x2728;</a>
        </p>
      </div>
    </div>
  </footer>
</body>

</html>